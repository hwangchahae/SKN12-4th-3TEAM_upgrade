"""
GitHub ì €ì¥ì†Œ ë¶„ì„ ë° ì„ë² ë”©ì„ ìœ„í•œ ê°œì„ ëœ ëª¨ë“ˆ

ì›ë³¸ github_analyzer.pyì˜ ì„±ëŠ¥ì„ ëŒ€í­ ê°œì„ í•œ ë²„ì „
ì£¼ìš” ê°œì„  ì‚¬í•­:
- ë³‘ë ¬ API í˜¸ì¶œ
- Trees API í™œìš©
- ìºì‹± ë©”ì»¤ë‹ˆì¦˜
- ë°°ì¹˜ ì²˜ë¦¬
"""

import requests
import chromadb
import os
import re
import openai
import base64
import asyncio
import aiohttp
from typing import Optional, List, Dict, Any, Tuple
from langchain.schema import Document
from cryptography.fernet import Fernet
import tiktoken
import ast
import markdown
import nbformat
from concurrent.futures import ThreadPoolExecutor, as_completed
import hashlib
import json
from functools import lru_cache
import time

# ----------------- ìƒìˆ˜ ì •ì˜ -----------------
MAIN_EXTENSIONS = ['.py', '.js', '.md', '.ts', '.java', '.cpp', '.h', '.hpp', '.c', '.cs', '.txt', '.ipynb']
CHUNK_SIZE = 500
GITHUB_TOKEN = "GITHUB_TOKEN"
KEY_FILE = ".key"
BATCH_SIZE = 20  # ë™ì‹œ ì²˜ë¦¬í•  íŒŒì¼ ìˆ˜
MAX_RETRIES = 3  # API í˜¸ì¶œ ì¬ì‹œë„ íšŸìˆ˜

# ChromaDB ì˜êµ¬ ì €ì¥ì†Œ í´ë¼ì´ì–¸íŠ¸
REPO_DB_PATH = "./repo_analysis_db"
os.makedirs(REPO_DB_PATH, exist_ok=True)
chroma_client = chromadb.PersistentClient(path=REPO_DB_PATH)

# ìºì‹œ ë””ë ‰í† ë¦¬
CACHE_DIR = "./github_cache"
os.makedirs(CACHE_DIR, exist_ok=True)


class GitHubRepositoryFetcherImproved:
    """
    ê°œì„ ëœ GitHub ì €ì¥ì†Œ íŒŒì¼ ê°€ì ¸ì˜¤ê¸° í´ë˜ìŠ¤
    
    ì„±ëŠ¥ ê°œì„  ì‚¬í•­:
    - ë³‘ë ¬ API í˜¸ì¶œ
    - Trees APIë¡œ í•œ ë²ˆì— íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
    - íŒŒì¼ ë‚´ìš© ìºì‹±
    - ë°°ì¹˜ ì²˜ë¦¬
    """
    
    def __init__(self, repo_url: str, token: Optional[str] = None, session_id: Optional[str] = None):
        """ì´ˆê¸°í™”"""
        self.repo_url = repo_url
        self.token = token or os.environ.get("GITHUB_TOKEN")
        self.headers = {
            'Authorization': f'token {self.token}',
            'Accept': 'application/vnd.github.v3+json',
            'User-Agent': 'GitHub-Code-Analyzer/2.0'
        } if self.token else {
            'Accept': 'application/vnd.github.v3+json',
            'User-Agent': 'GitHub-Code-Analyzer/2.0'
        }
        
        self.files = []
        self.api_call_count = 0
        self.cache_hits = 0
        self.cache_misses = 0
        
        # ì €ì¥ì†Œ ì •ë³´ ì¶”ì¶œ
        self.owner, self.repo, self.path = self.extract_repo_info(repo_url)
        if not self.owner or not self.repo:
            raise ValueError("Invalid GitHub repository URL")
        
        # ì„¸ì…˜ ì„¤ì •
        self.session_id = session_id or f"{self.owner}_{self.repo}"
        self.repo_path = f"./repos/{self.session_id}"
        
        # ìºì‹œ ì„¤ì •
        self.cache_file = os.path.join(CACHE_DIR, f"{self.owner}_{self.repo}_cache.json")
        self.file_cache = self.load_cache()
        
        # ChromaDB ì»¬ë ‰ì…˜
        self.collection = chroma_client.get_or_create_collection(
            name=self.session_id,
            metadata={"description": f"Repository: {self.owner}/{self.repo}"}
        )
    
    def extract_repo_info(self, url: str) -> Tuple[Optional[str], Optional[str], Optional[str]]:
        """GitHub URLì—ì„œ ì†Œìœ ì, ì €ì¥ì†Œ ì´ë¦„, íŒŒì¼ ê²½ë¡œë¥¼ ì¶”ì¶œ"""
        try:
            url = url.strip().rstrip('/')
            if url.endswith('.git'):
                url = url[:-4]
            
            parts = url.split('/')
            if 'github.com' in parts:
                github_index = parts.index('github.com')
                if len(parts) >= github_index + 3:
                    owner = parts[github_index + 1]
                    repo = parts[github_index + 2]
                    path = '/'.join(parts[github_index + 3:]) if len(parts) > github_index + 3 else None
                    return owner, repo, path
        except Exception as e:
            print(f"URL íŒŒì‹± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None, None, None
    
    def load_cache(self) -> Dict[str, Any]:
        """ìºì‹œ íŒŒì¼ ë¡œë“œ"""
        if os.path.exists(self.cache_file):
            try:
                with open(self.cache_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except:
                return {}
        return {}
    
    def save_cache(self):
        """ìºì‹œ íŒŒì¼ ì €ì¥"""
        try:
            with open(self.cache_file, 'w', encoding='utf-8') as f:
                json.dump(self.file_cache, f, ensure_ascii=False)
        except Exception as e:
            print(f"ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def get_cache_key(self, path: str, sha: Optional[str] = None) -> str:
        """ìºì‹œ í‚¤ ìƒì„±"""
        if sha:
            return f"{path}:{sha}"
        return path
    
    def filter_main_files_fast(self):
        """
        Trees APIë¥¼ ì‚¬ìš©í•˜ì—¬ í•œ ë²ˆì˜ í˜¸ì¶œë¡œ ëª¨ë“  íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
        """
        print(f"[DEBUG] Trees APIë¡œ íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°...")
        
        # ê¸°ë³¸ ë¸Œëœì¹˜ í™•ì¸
        repo_url = f"https://api.github.com/repos/{self.owner}/{self.repo}"
        response = requests.get(repo_url, headers=self.headers)
        self.api_call_count += 1
        
        if response.status_code != 200:
            print(f"[ERROR] ì €ì¥ì†Œ ì •ë³´ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {response.status_code}")
            return
        
        default_branch = response.json().get('default_branch', 'main')
        
        # Trees APIë¡œ ì „ì²´ íŒŒì¼ íŠ¸ë¦¬ ê°€ì ¸ì˜¤ê¸°
        trees_url = f"https://api.github.com/repos/{self.owner}/{self.repo}/git/trees/{default_branch}?recursive=1"
        response = requests.get(trees_url, headers=self.headers)
        self.api_call_count += 1
        
        if response.status_code != 200:
            print(f"[ERROR] Trees API í˜¸ì¶œ ì‹¤íŒ¨: {response.status_code}")
            # í´ë°±: ê¸°ì¡´ ë°©ì‹ ì‚¬ìš©
            self.filter_main_files()
            return
        
        tree_data = response.json()
        
        # ì£¼ìš” íŒŒì¼ë§Œ í•„í„°ë§
        self.files = []
        for item in tree_data.get('tree', []):
            if item['type'] == 'blob':  # íŒŒì¼ì¸ ê²½ìš°
                path = item['path']
                if any(path.endswith(ext) for ext in MAIN_EXTENSIONS):
                    # í° íŒŒì¼ ì œì™¸
                    if item.get('size', 0) < 100000:  # 100KB ë¯¸ë§Œ
                        self.files.append({
                            'path': path,
                            'sha': item['sha'],
                            'size': item.get('size', 0)
                        })
        
        print(f"[DEBUG] Trees APIë¡œ {len(self.files)}ê°œ íŒŒì¼ í•„í„°ë§ ì™„ë£Œ")
    
    def filter_main_files(self):
        """ê¸°ì¡´ ë°©ì‹ (í´ë°±ìš©)"""
        self.files = self.get_all_main_files()
        print(f"[DEBUG] ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ {len(self.files)}ê°œ íŒŒì¼ í•„í„°ë§")
    
    def get_all_main_files(self, path=""):
        """ê¸°ì¡´ ì¬ê·€ì  íŒŒì¼ íƒìƒ‰ (í´ë°±ìš©)"""
        files = []
        dir_contents = self.get_repo_directory_contents(path)
        
        if isinstance(dir_contents, dict) and dir_contents.get('error'):
            return files
        
        if isinstance(dir_contents, list):
            for item in dir_contents:
                if item['type'] == 'file' and any(item['path'].endswith(ext) for ext in MAIN_EXTENSIONS):
                    files.append(item['path'])
                elif item['type'] == 'dir':
                    sub_files = self.get_all_main_files(item['path'])
                    files.extend(sub_files)
        
        return files
    
    def get_repo_directory_contents(self, path: str = "") -> Any:
        """ë””ë ‰í† ë¦¬ ë‚´ìš© ê°€ì ¸ì˜¤ê¸° (í´ë°±ìš©)"""
        try:
            url = f"https://api.github.com/repos/{self.owner}/{self.repo}/contents/{path}"
            response = requests.get(url, headers=self.headers)
            self.api_call_count += 1
            
            if response.status_code == 200:
                return response.json()
            return {"error": True, "status_code": response.status_code}
        except Exception as e:
            return {"error": True, "message": str(e)}
    
    async def fetch_file_content_async(self, session: aiohttp.ClientSession, file_info: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """ë¹„ë™ê¸°ë¡œ íŒŒì¼ ë‚´ìš© ê°€ì ¸ì˜¤ê¸°"""
        path = file_info['path'] if isinstance(file_info, dict) else file_info
        sha = file_info.get('sha') if isinstance(file_info, dict) else None
        
        # ìºì‹œ í™•ì¸
        cache_key = self.get_cache_key(path, sha)
        if cache_key in self.file_cache:
            self.cache_hits += 1
            cached_data = self.file_cache[cache_key]
            # ìºì‹œëœ ë°ì´í„°ê°€ 1ì¼ ì´ë‚´ì¸ ê²½ìš° ì‚¬ìš©
            if time.time() - cached_data.get('timestamp', 0) < 86400:
                return cached_data['content']
        
        self.cache_misses += 1
        
        # API í˜¸ì¶œ
        url = f"https://api.github.com/repos/{self.owner}/{self.repo}/contents/{path}"
        
        for retry in range(MAX_RETRIES):
            try:
                async with session.get(url, headers=self.headers) as response:
                    self.api_call_count += 1
                    
                    if response.status == 200:
                        data = await response.json()
                        
                        # Base64 ë””ì½”ë”©
                        try:
                            content = base64.b64decode(data['content']).decode('utf-8')
                            
                            result = {
                                'path': path,
                                'content': content,
                                'file_name': data['name'],
                                'file_type': data['name'].split('.')[-1] if '.' in data['name'] else '',
                                'sha': data['sha'],
                                'source_url': data['html_url']
                            }
                            
                            # ìºì‹œ ì €ì¥
                            self.file_cache[cache_key] = {
                                'content': result,
                                'timestamp': time.time()
                            }
                            
                            return result
                            
                        except UnicodeDecodeError:
                            # ë°”ì´ë„ˆë¦¬ íŒŒì¼
                            return None
                    
                    elif response.status == 403 and retry < MAX_RETRIES - 1:
                        # API ì œí•œ - ì ì‹œ ëŒ€ê¸° í›„ ì¬ì‹œë„
                        await asyncio.sleep(2 ** retry)
                    else:
                        return None
                        
            except Exception as e:
                if retry == MAX_RETRIES - 1:
                    print(f"[ERROR] íŒŒì¼ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨ ({path}): {e}")
                    return None
                await asyncio.sleep(1)
        
        return None
    
    async def get_file_contents_parallel(self) -> List[Dict[str, Any]]:
        """ë³‘ë ¬ë¡œ íŒŒì¼ ë‚´ìš© ê°€ì ¸ì˜¤ê¸°"""
        print(f"[DEBUG] ë³‘ë ¬ ì²˜ë¦¬ ì‹œì‘: {len(self.files)}ê°œ íŒŒì¼")
        
        # Trees API ì‚¬ìš© (ë” ë¹ ë¥¸ íŒŒì¼ ëª©ë¡)
        if not self.files:
            self.filter_main_files_fast()
        
        file_objs = []
        
        # aiohttp ì„¸ì…˜ ìƒì„±
        connector = aiohttp.TCPConnector(limit=BATCH_SIZE)
        timeout = aiohttp.ClientTimeout(total=30)
        
        async with aiohttp.ClientSession(connector=connector, timeout=timeout) as session:
            # ë°°ì¹˜ ì²˜ë¦¬
            for i in range(0, len(self.files), BATCH_SIZE):
                batch = self.files[i:i + BATCH_SIZE]
                
                # ë°°ì¹˜ ë‚´ íŒŒì¼ë“¤ì„ ë³‘ë ¬ë¡œ ê°€ì ¸ì˜¤ê¸°
                tasks = [self.fetch_file_content_async(session, file_info) for file_info in batch]
                results = await asyncio.gather(*tasks)
                
                # ê²°ê³¼ ìˆ˜ì§‘
                for result in results:
                    if result:
                        file_objs.append(result)
                
                # ì§„í–‰ ìƒí™© í‘œì‹œ
                processed = min(i + BATCH_SIZE, len(self.files))
                print(f"[DEBUG] ì§„í–‰: {processed}/{len(self.files)} íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ")
                
                # API ì œí•œ ë°©ì§€ë¥¼ ìœ„í•œ ì§§ì€ ëŒ€ê¸°
                if i + BATCH_SIZE < len(self.files):
                    await asyncio.sleep(0.1)
        
        # ìºì‹œ ì €ì¥
        self.save_cache()
        
        print(f"[DEBUG] ë³‘ë ¬ ì²˜ë¦¬ ì™„ë£Œ: {len(file_objs)}ê°œ íŒŒì¼")
        print(f"[DEBUG] API í˜¸ì¶œ: {self.api_call_count}íšŒ")
        print(f"[DEBUG] ìºì‹œ íˆíŠ¸: {self.cache_hits}, ë¯¸ìŠ¤: {self.cache_misses}")
        
        return file_objs
    
    def get_file_contents_sequential(self) -> List[Dict[str, Any]]:
        """ìˆœì°¨ ì²˜ë¦¬ (ì‘ì€ ì €ì¥ì†Œìš©)"""
        print(f"[DEBUG] ìˆœì°¨ ì²˜ë¦¬ ì‹œì‘: {len(self.files)}ê°œ íŒŒì¼")
        
        if not self.files:
            self.filter_main_files_fast()
        
        file_objs = []
        
        for file_info in self.files:
            path = file_info['path'] if isinstance(file_info, dict) else file_info
            sha = file_info.get('sha') if isinstance(file_info, dict) else None
            
            # ìºì‹œ í™•ì¸
            cache_key = self.get_cache_key(path, sha)
            if cache_key in self.file_cache:
                self.cache_hits += 1
                cached_data = self.file_cache[cache_key]
                if time.time() - cached_data.get('timestamp', 0) < 86400:
                    file_objs.append(cached_data['content'])
                    continue
            
            self.cache_misses += 1
            
            # API í˜¸ì¶œ
            url = f"https://api.github.com/repos/{self.owner}/{self.repo}/contents/{path}"
            response = requests.get(url, headers=self.headers)
            self.api_call_count += 1
            
            if response.status_code == 200:
                data = response.json()
                try:
                    content = base64.b64decode(data['content']).decode('utf-8')
                    result = {
                        'path': path,
                        'content': content,
                        'file_name': data['name'],
                        'file_type': data['name'].split('.')[-1] if '.' in data['name'] else '',
                        'sha': data['sha'],
                        'source_url': data['html_url']
                    }
                    
                    # ìºì‹œ ì €ì¥
                    self.file_cache[cache_key] = {
                        'content': result,
                        'timestamp': time.time()
                    }
                    
                    file_objs.append(result)
                except UnicodeDecodeError:
                    pass  # ë°”ì´ë„ˆë¦¬ íŒŒì¼ ê±´ë„ˆë›°ê¸°
        
        # ìºì‹œ ì €ì¥
        self.save_cache()
        
        print(f"[DEBUG] ìˆœì°¨ ì²˜ë¦¬ ì™„ë£Œ: {len(file_objs)}ê°œ íŒŒì¼")
        print(f"[DEBUG] API í˜¸ì¶œ: {self.api_call_count}íšŒ")
        print(f"[DEBUG] ìºì‹œ íˆíŠ¸: {self.cache_hits}, ë¯¸ìŠ¤: {self.cache_misses}")
        
        return file_objs
    
    def get_file_contents(self) -> List[Dict[str, Any]]:
        """ìŠ¤ë§ˆíŠ¸ íŒŒì¼ ê°€ì ¸ì˜¤ê¸° (íŒŒì¼ ìˆ˜ì— ë”°ë¼ ìë™ ì„ íƒ)"""
        
        # íŒŒì¼ ëª©ë¡ì´ ì—†ìœ¼ë©´ ë¨¼ì € ê°€ì ¸ì˜¤ê¸°
        if not self.files:
            self.filter_main_files_fast()
        
        file_count = len(self.files)
        
        # ì „ëµ ì„ íƒ ì„ê³„ê°’
        SEQUENTIAL_THRESHOLD = 5    # 5ê°œ ë¯¸ë§Œ: ìˆœì°¨ ì²˜ë¦¬
        GRAPHQL_THRESHOLD = 500     # 500ê°œ ì´ìƒ: GraphQL ì‚¬ìš©
        
        print(f"[DEBUG] íŒŒì¼ ìˆ˜: {file_count}ê°œ")
        
        if file_count < SEQUENTIAL_THRESHOLD:
            # ì‘ì€ ì €ì¥ì†Œ: ìˆœì°¨ ì²˜ë¦¬ê°€ ë” ë¹ ë¦„
            print(f"[DEBUG] {SEQUENTIAL_THRESHOLD}ê°œ ë¯¸ë§Œ -> ìˆœì°¨ ì²˜ë¦¬ ì„ íƒ")
            return self.get_file_contents_sequential()
        elif file_count >= GRAPHQL_THRESHOLD and self.token:
            # ëŒ€ìš©ëŸ‰ ì €ì¥ì†Œ: GraphQL ì‚¬ìš© (í† í° í•„ìš”)
            print(f"[DEBUG] {GRAPHQL_THRESHOLD}ê°œ ì´ìƒ -> GraphQL ëª¨ë“œ ì„ íƒ")
            try:
                from github_analyzer_graphql import GitHubGraphQLFetcher
                graphql_fetcher = GitHubGraphQLFetcher(self.repo_url, self.token)
                files = graphql_fetcher.get_all_files_optimized()
                self.api_call_count += graphql_fetcher.api_call_count
                return files
            except ImportError:
                print(f"[DEBUG] GraphQL ëª¨ë“ˆ ì—†ìŒ -> ë³‘ë ¬ ì²˜ë¦¬ë¡œ ëŒ€ì²´")
                # GraphQL ëª¨ë“ˆì´ ì—†ìœ¼ë©´ ë³‘ë ¬ ì²˜ë¦¬ë¡œ ëŒ€ì²´
        
        # ì¤‘ê°„ í¬ê¸° ì €ì¥ì†Œ: ë³‘ë ¬ ì²˜ë¦¬ê°€ ìµœì 
        print(f"[DEBUG] {SEQUENTIAL_THRESHOLD}ê°œ ì´ìƒ {GRAPHQL_THRESHOLD}ê°œ ë¯¸ë§Œ -> ë³‘ë ¬ ì²˜ë¦¬ ì„ íƒ")
        # ì´ë²¤íŠ¸ ë£¨í”„ê°€ ì´ë¯¸ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸
        try:
            loop = asyncio.get_running_loop()
            # ì´ë¯¸ ë£¨í”„ê°€ ì‹¤í–‰ ì¤‘ì´ë©´ ìƒˆ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰
            import concurrent.futures
            with concurrent.futures.ThreadPoolExecutor() as executor:
                future = executor.submit(asyncio.run, self.get_file_contents_parallel())
                return future.result()
        except RuntimeError:
            # ë£¨í”„ê°€ ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±
            return asyncio.run(self.get_file_contents_parallel())
    
    def get_directory_structure(self) -> str:
        """ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„±"""
        if not self.files:
            self.filter_main_files_fast()
        
        # íŒŒì¼ ê²½ë¡œë¡œ íŠ¸ë¦¬ êµ¬ì¡° ìƒì„±
        tree = {}
        for file_info in self.files:
            path = file_info['path'] if isinstance(file_info, dict) else file_info
            parts = path.split('/')
            
            current = tree
            for i, part in enumerate(parts):
                if i == len(parts) - 1:
                    # íŒŒì¼
                    current[f"ğŸ“„ {part}"] = None
                else:
                    # ë””ë ‰í† ë¦¬
                    if f"ğŸ“ {part}" not in current:
                        current[f"ğŸ“ {part}"] = {}
                    current = current[f"ğŸ“ {part}"]
        
        # íŠ¸ë¦¬ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
        lines = []
        def traverse(node, prefix=""):
            for key, value in sorted(node.items()):
                lines.append(f"{prefix}{key}")
                if value is not None:
                    traverse(value, prefix + "  ")
        
        traverse(tree)
        return "\n".join(lines)
    
    def load_repo_data(self) -> bool:
        """ì €ì¥ì†Œ ë°ì´í„° ë¡œë“œ"""
        try:
            if self.files:
                return True
            
            # Trees API ì‚¬ìš© (ë” ë¹ ë¦„)
            self.filter_main_files_fast()
            
            if not self.files:
                print("[WARNING] í•„í„°ë§ëœ íŒŒì¼ ëª©ë¡ì´ ì—†ìŠµë‹ˆë‹¤.")
                return False
            
            print(f"[DEBUG] ë°ì´í„° ë¡œë“œ ì„±ê³µ: {len(self.files)} íŒŒì¼")
            return True
            
        except Exception as e:
            print(f"[ERROR] ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False
    
    # í† í° ê´€ë¦¬ ë©”ì„œë“œë“¤ (ê¸°ì¡´ê³¼ ë™ì¼)
    @staticmethod
    def generate_key() -> bytes:
        """ì•”í˜¸í™” í‚¤ ìƒì„±"""
        if not os.path.exists(KEY_FILE):
            key = Fernet.generate_key()
            with open(KEY_FILE, 'wb') as key_file:
                key_file.write(key)
            return key
        else:
            with open(KEY_FILE, 'rb') as key_file:
                return key_file.read()
    
    @staticmethod
    def encrypt_token(token: str) -> str:
        """í† í° ì•”í˜¸í™”"""
        key = GitHubRepositoryFetcherImproved.generate_key()
        f = Fernet(key)
        return f.encrypt(token.encode()).decode()
    
    @staticmethod
    def decrypt_token(encrypted_token: str) -> str:
        """í† í° ë³µí˜¸í™”"""
        key = GitHubRepositoryFetcherImproved.generate_key()
        f = Fernet(key)
        return f.decrypt(encrypted_token.encode()).decode()


def analyze_repository(repo_url: str, token: Optional[str] = None, session_id: Optional[str] = None) -> Dict[str, Any]:
    """
    ê°œì„ ëœ ì €ì¥ì†Œ ë¶„ì„ í•¨ìˆ˜
    
    íŒŒì¼ ìˆ˜ì— ë”°ë¼ ìë™ìœ¼ë¡œ ìµœì  ì „ëµ ì„ íƒ:
    - 5ê°œ ë¯¸ë§Œ: ìˆœì°¨ ì²˜ë¦¬ (ì˜¤ë²„í—¤ë“œ ìµœì†Œí™”)
    - 5ê°œ ì´ìƒ: ë³‘ë ¬ ì²˜ë¦¬ (ì„±ëŠ¥ ê·¹ëŒ€í™”)
    """
    try:
        print(f"[DEBUG] ìŠ¤ë§ˆíŠ¸ ì €ì¥ì†Œ ë¶„ì„ ì‹œì‘: {repo_url}")
        
        # ChromaDB ë””ë ‰í† ë¦¬ ì •ë¦¬
        if session_id:
            cleanup_chromadb_for_session(session_id)
        
        # ê°œì„ ëœ Fetcher ì‚¬ìš©
        fetcher = GitHubRepositoryFetcherImproved(repo_url, token, session_id)
        
        # ë°ì´í„° ë¡œë“œ
        if not fetcher.load_repo_data():
            return {'success': False, 'error': 'ì €ì¥ì†Œ ë°ì´í„°ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'}
        
        # ìŠ¤ë§ˆíŠ¸ íŒŒì¼ ê°€ì ¸ì˜¤ê¸° (íŒŒì¼ ìˆ˜ì— ë”°ë¼ ìë™ ì„ íƒ)
        files = fetcher.get_file_contents()  # ë‚´ë¶€ì—ì„œ ìë™ìœ¼ë¡œ ìˆœì°¨/ë³‘ë ¬ ì„ íƒ
        
        if not files:
            return {'success': False, 'error': 'ì €ì¥ì†Œì—ì„œ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'}
        
        # ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„±
        directory_structure = fetcher.get_directory_structure()
        
        print(f"[DEBUG] íŒŒì¼ ìˆ˜ì§‘ ì™„ë£Œ: {len(files)} íŒŒì¼")
        print(f"[DEBUG] API í˜¸ì¶œ íšŸìˆ˜: {fetcher.api_call_count}")
        print(f"[DEBUG] ìºì‹œ íš¨ìœ¨: {fetcher.cache_hits}/{fetcher.cache_hits + fetcher.cache_misses}")
        
        # ì„ë² ë”© ì²˜ë¦¬ (ê¸°ì¡´ê³¼ ë™ì¼)
        if session_id:
            from github_analyzer import RepositoryEmbedder
            embedder = RepositoryEmbedder(session_id)
            embedder.process_and_embed(files)
            print(f"[DEBUG] ì„ë² ë”© ì²˜ë¦¬ ì™„ë£Œ")
        
        return {
            'success': True,
            'files': files,
            'directory_structure': directory_structure,
            'total_files': len(files),
            'api_calls': fetcher.api_call_count,
            'cache_hits': fetcher.cache_hits,
            'cache_misses': fetcher.cache_misses
        }
        
    except Exception as e:
        import traceback
        print(f"[ERROR] ì €ì¥ì†Œ ë¶„ì„ ì‹¤íŒ¨: {e}")
        traceback.print_exc()
        return {'success': False, 'error': f'ì €ì¥ì†Œ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}'}


def cleanup_chromadb_for_session(session_id: str):
    """ì„¸ì…˜ì˜ ChromaDB ë°ì´í„° ì •ë¦¬"""
    try:
        import shutil
        chroma_path = os.path.join("repo_analysis_db", session_id)
        if os.path.exists(chroma_path):
            shutil.rmtree(chroma_path)
    except Exception as e:
        print(f"[WARNING] ChromaDB ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")


# ê¸°ì¡´ í•¨ìˆ˜ë“¤ê³¼ì˜ í˜¸í™˜ì„± ìœ ì§€
def get_repository_branches(repo_url: str, token: Optional[str] = None) -> Dict[str, Any]:
    """ë¸Œëœì¹˜ ëª©ë¡ ê°€ì ¸ì˜¤ê¸° (ê¸°ì¡´ê³¼ ë™ì¼)"""
    from github_analyzer import get_repository_branches as original_func
    return original_func(repo_url, token)


def get_repository_file_tree(repo_url: str, branch: str = 'main', token: Optional[str] = None) -> Dict[str, Any]:
    """íŒŒì¼ íŠ¸ë¦¬ ê°€ì ¸ì˜¤ê¸° (ê¸°ì¡´ê³¼ ë™ì¼)"""
    from github_analyzer import get_repository_file_tree as original_func
    return original_func(repo_url, branch, token)


def get_file_content(repo_url: str, file_path: str, branch: str = 'main', token: Optional[str] = None) -> Dict[str, Any]:
    """íŒŒì¼ ë‚´ìš© ê°€ì ¸ì˜¤ê¸° (ê¸°ì¡´ê³¼ ë™ì¼)"""
    from github_analyzer import get_file_content as original_func
    return original_func(repo_url, file_path, branch, token)


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸
    print("=" * 60)
    print("GitHub Analyzer Improved - ìŠ¤ë§ˆíŠ¸ ì²˜ë¦¬ ì „ëµ")
    print("=" * 60)
    print("íŒŒì¼ ìˆ˜ì— ë”°ë¼ ìë™ìœ¼ë¡œ ìµœì  ì „ëµ ì„ íƒ:")
    print("  - 1-4ê°œ íŒŒì¼: ìˆœì°¨ ì²˜ë¦¬ (ì˜¤ë²„í—¤ë“œ ìµœì†Œí™”)")
    print("  - 5ê°œ ì´ìƒ: ë³‘ë ¬ ì²˜ë¦¬ (ì„±ëŠ¥ ê·¹ëŒ€í™”)")
    print("=" * 60)
    
    test_repo = "https://github.com/octocat/Hello-World"
    print(f"\ní…ŒìŠ¤íŠ¸ ì €ì¥ì†Œ: {test_repo}")
    
    result = analyze_repository(test_repo)
    if result['success']:
        print(f"\në¶„ì„ ì„±ê³µ!")
        print(f"  - íŒŒì¼ ìˆ˜: {result['total_files']}")
        print(f"  - API í˜¸ì¶œ: {result.get('api_calls', 'N/A')}")
        print(f"  - ìºì‹œ íˆíŠ¸: {result.get('cache_hits', 'N/A')}")
        print(f"  - ì²˜ë¦¬ ì „ëµ: {'ìˆœì°¨' if result['total_files'] < 5 else 'ë³‘ë ¬'} ì²˜ë¦¬ ì‚¬ìš©")
    else:
        print(f"ë¶„ì„ ì‹¤íŒ¨: {result['error']}")